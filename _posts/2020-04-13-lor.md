# Analysis of Social Network Advertisements by Logistic Regression

Below, I wish to use the logistic regression algorthim to analyze the effectiveness of social network advertisements. In this case, our dependent variable is whether or not a user purchased the item, in this case, a luxury SUV, after seeing the advertisement. This feedback is crucial to those in marketing and sales departments. The independent variables are a combination of demographic features. This algorithm will answer the question which features have the greatest impact on a user clicking the ad. This, in turn, will allow for the company to selectively target their audience and maximizing their budgets. 

This ability to selectively target audiences is the reason that CPM and CPC rates have continuously increased on social networking sites. The sites provide the ability to target and the firms pay higher rates. 

Let's see what this really looks like. 

## Load Libraries


```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
```

## Read in the Data


```python
df = pd.read_csv('Social_Network_Ads.csv')
df.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>User ID</th>
      <th>Gender</th>
      <th>Age</th>
      <th>EstimatedSalary</th>
      <th>Purchased</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>15624510</td>
      <td>Male</td>
      <td>19</td>
      <td>19000</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>15810944</td>
      <td>Male</td>
      <td>35</td>
      <td>20000</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>15668575</td>
      <td>Female</td>
      <td>26</td>
      <td>43000</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>15603246</td>
      <td>Female</td>
      <td>27</td>
      <td>57000</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>15804002</td>
      <td>Male</td>
      <td>19</td>
      <td>76000</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>




```python
df.isnull().sum()
```




    User ID            0
    Gender             0
    Age                0
    EstimatedSalary    0
    Purchased          0
    dtype: int64




```python
df.info()
```

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 400 entries, 0 to 399
    Data columns (total 5 columns):
     #   Column           Non-Null Count  Dtype 
    ---  ------           --------------  ----- 
     0   User ID          400 non-null    int64 
     1   Gender           400 non-null    object
     2   Age              400 non-null    int64 
     3   EstimatedSalary  400 non-null    int64 
     4   Purchased        400 non-null    int64 
    dtypes: int64(4), object(1)
    memory usage: 15.8+ KB


From the imported data, we see that we have 5 variables with no null values. They are User ID, Gender, Age, Estimated Salary, and Purchased. User ID is most likely a Primary Key and is not relevant to our analysis. Gender, Age, and Estimated Salary will be our independent variables that we will use to explain the dependent variable of whether or not the view of the advertisement purchased the luxury SUV. 

## Data Cleaning

First, we notice that our Gender column is of type "object" which is not suitable for this algorithm. We will encode it as 1 for male and 0 for female. We will remove the User ID column. 

We will also  define X as our matrix of features and y as our dependent variable. We will do this after exploring the data. 


```python
# Binary encoding of Gender variable
df["Gender"] = df['Gender'].replace({"Female":0, "Male":1})

# Omitting User Id
df = df.iloc[:,1:5]

# View new dataframe
df
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Gender</th>
      <th>Age</th>
      <th>EstimatedSalary</th>
      <th>Purchased</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>19</td>
      <td>19000</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>35</td>
      <td>20000</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0</td>
      <td>26</td>
      <td>43000</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0</td>
      <td>27</td>
      <td>57000</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1</td>
      <td>19</td>
      <td>76000</td>
      <td>0</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>395</th>
      <td>0</td>
      <td>46</td>
      <td>41000</td>
      <td>1</td>
    </tr>
    <tr>
      <th>396</th>
      <td>1</td>
      <td>51</td>
      <td>23000</td>
      <td>1</td>
    </tr>
    <tr>
      <th>397</th>
      <td>0</td>
      <td>50</td>
      <td>20000</td>
      <td>1</td>
    </tr>
    <tr>
      <th>398</th>
      <td>1</td>
      <td>36</td>
      <td>33000</td>
      <td>0</td>
    </tr>
    <tr>
      <th>399</th>
      <td>0</td>
      <td>49</td>
      <td>36000</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
<p>400 rows Ã— 4 columns</p>
</div>



Data Cleaning is complete for now. As mentioned above, we will scale the features, seperate independent and dependent variables, and split data into test and train set in the modelling section. 

## Data Exploration

In this section, I want to get a feel for what could be a factor in purchasing a luxury SUV. Luxury tells me that this is a car that perhaps only those within a certain income range can afford. Let us tak a closer look at income and purchases. 


```python
sns.distplot(df["EstimatedSalary"])
```




    <matplotlib.axes._subplots.AxesSubplot at 0x1a1b1d35d0>




![png](output_15_1.png)


The histogram looks approximately normal. 


```python
sns.set(style="whitegrid")
sns.boxplot(x="Purchased", y="EstimatedSalary", data=df)
```




    <matplotlib.axes._subplots.AxesSubplot at 0x1a1b1d9550>




![png](output_17_1.png)


From this boxplot, it is intuitively obvious that the group who purchased the car had a higher average income than those who did not. Let's look at this by gender, too.


```python
males = df["Gender"]==1
females = df["Gender"]==0

df_males = df[males]
df_females = df[females]

plt.subplot(1, 2, 1)
sns.boxplot(x="Purchased", y="EstimatedSalary", data=df_males)
plt.subplot(1, 2, 2)
sns.boxplot(x="Purchased", y="EstimatedSalary", data=df_females)
plt.tight_layout()

```


![png](output_19_0.png)


The boxplot look almost identical between males and females, a possible indication of no correlation between Gender and Purchase. Lets look at only Gender and see how that has a role in Purchase. To do this, we will look at the probability of purchase given male, then given female.


```python
df_males["Purchased"].mean()*100
```




    33.6734693877551




```python
df_females["Purchased"].mean()*100
```




    37.745098039215684



The probabilities are very similar. This implies that Gender, most likely, does not have a correlation with whether or not the SUV was purchased. Now we will explore the Age variable. 


```python
sns.distplot(df["Age"])
```




    <matplotlib.axes._subplots.AxesSubplot at 0x1a1b25d790>




![png](output_24_1.png)


Age is normally distributed. This is usually what we would want in a data sample yet in an age feature, uniform might be preferred. One manner of doing this would be to discretize age. In doing so, one would create bins of the same size. I am electing not to do that here as I believe that this distribution is representative of the market.


```python
sns.boxplot(x="Purchased", y="Age", data=df)
```




    <matplotlib.axes._subplots.AxesSubplot at 0x1a1a0fd590>




![png](output_26_1.png)


From the boxplot, it is clear that those over 40 are more likely to Purchase the luxury SUV. Lastly, I wish to explore the relationship that Age and Income may have on each other. It is possible that this may result in an issue of multicolinearity. 


```python
plt.scatter(x="Age", y="EstimatedSalary", data=df)
plt.title("Multicolinearity Check: Age and Salary")
plt.show()
```


![png](output_28_0.png)


From the scatterplot, there is no obvious functional correlation between the two variables. 

## Modeling 

Now that we have explored the data, we will separate it into the features we wish to use as independent(X) and dependent(y) variables. We will omit age as it did not have a signficant correlation with the dependent variable. 


```python
X = df.iloc[:, [1, 2]].values
y = df.iloc[:, 3].values
```


```python
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)
```

Now we will scale the data via the Standard Scaler class from the sklearn library. From the documentation, this will convert terms to a z score, such that z = (x - u) / s; where x = individual observation, u = mean of all x's, and s = standard deviation of the x's. 


```python
from sklearn.preprocessing import StandardScaler
ss = StandardScaler()
X_train = ss.fit_transform(X_train)
X_test = ss.transform(X_test)
```

Now that we have split and scaled our data, we are ready to import and fit the model. Then use that model for predictions. 


```python
from sklearn.linear_model import LogisticRegression
model = LogisticRegression()
model.fit(X_train, y_train)
```




    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                       intercept_scaling=1, l1_ratio=None, max_iter=100,
                       multi_class='auto', n_jobs=None, penalty='l2',
                       random_state=None, solver='lbfgs', tol=0.0001, verbose=0,
                       warm_start=False)




```python
model.score(X_test, y_test)
```




    0.9




```python
predictions = model.predict(X_test)
```


```python
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, predictions)
cm
```




    array([[76,  6],
           [ 6, 32]])



## Conclusion

Our model score, or accuracy, was 91%. The model correctly classified 109 observation and misclassified 11 of them. The objective of this model was to predict whether one might buy a luxury SUV by one's age and salary. That is what this model has done with a great deal of accuracy. If one needs more information about which one of these variables is more important in determining the Purchase or if it is a significant indicator, statsmodel library and a logit regression call would be the next steps. 