---
title: "Polynomial Features in Linear Regression: Predicting the Quality of Wine"
date: 2020-04-25
tags: [Regression]
header: 
    image: "/images/headerpf.jpg"
excerpt: "Exploring polynomial features: Can it improve wine quality model?"
---

# An Exploration in Polynomial Features

### Will added polynomial features improve a linear regression model?

In this project, I want to go deeper into linear regression. Python has a class called "Polynomial Features" in the ever more popular scikit-learn library. Most of my experience in regression analysis has been limited to simple linear regression or multiple linear regression. I wanted to create a multiple linear regression model, and discover whether adding polynomial features would improve its performance. I recognize that this question is dataset dependent. In this project, I wish to lay out a general path to answer the question.

## Import the libraries


```python
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
```

## Import the data

The data for this project is downloaded from Kaggle and can be found [here.](https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009) It is a wide dataset that has various features. I will see with what precison a few models can predict the quality of these wine given the features. 


```python
df = pd.read_csv("winequality-red.csv")
df.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>fixed acidity</th>
      <th>volatile acidity</th>
      <th>citric acid</th>
      <th>residual sugar</th>
      <th>chlorides</th>
      <th>free sulfur dioxide</th>
      <th>total sulfur dioxide</th>
      <th>density</th>
      <th>pH</th>
      <th>sulphates</th>
      <th>alcohol</th>
      <th>quality</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>7.4</td>
      <td>0.70</td>
      <td>0.00</td>
      <td>1.9</td>
      <td>0.076</td>
      <td>11.0</td>
      <td>34.0</td>
      <td>0.9978</td>
      <td>3.51</td>
      <td>0.56</td>
      <td>9.4</td>
      <td>5</td>
    </tr>
    <tr>
      <th>1</th>
      <td>7.8</td>
      <td>0.88</td>
      <td>0.00</td>
      <td>2.6</td>
      <td>0.098</td>
      <td>25.0</td>
      <td>67.0</td>
      <td>0.9968</td>
      <td>3.20</td>
      <td>0.68</td>
      <td>9.8</td>
      <td>5</td>
    </tr>
    <tr>
      <th>2</th>
      <td>7.8</td>
      <td>0.76</td>
      <td>0.04</td>
      <td>2.3</td>
      <td>0.092</td>
      <td>15.0</td>
      <td>54.0</td>
      <td>0.9970</td>
      <td>3.26</td>
      <td>0.65</td>
      <td>9.8</td>
      <td>5</td>
    </tr>
    <tr>
      <th>3</th>
      <td>11.2</td>
      <td>0.28</td>
      <td>0.56</td>
      <td>1.9</td>
      <td>0.075</td>
      <td>17.0</td>
      <td>60.0</td>
      <td>0.9980</td>
      <td>3.16</td>
      <td>0.58</td>
      <td>9.8</td>
      <td>6</td>
    </tr>
    <tr>
      <th>4</th>
      <td>7.4</td>
      <td>0.70</td>
      <td>0.00</td>
      <td>1.9</td>
      <td>0.076</td>
      <td>11.0</td>
      <td>34.0</td>
      <td>0.9978</td>
      <td>3.51</td>
      <td>0.56</td>
      <td>9.4</td>
      <td>5</td>
    </tr>
  </tbody>
</table>
</div>




```python
df.isnull().sum()
```




    fixed acidity           0
    volatile acidity        0
    citric acid             0
    residual sugar          0
    chlorides               0
    free sulfur dioxide     0
    total sulfur dioxide    0
    density                 0
    pH                      0
    sulphates               0
    alcohol                 0
    quality                 0
    dtype: int64




```python
df.info()
```

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 1599 entries, 0 to 1598
    Data columns (total 12 columns):
     #   Column                Non-Null Count  Dtype  
    ---  ------                --------------  -----  
     0   fixed acidity         1599 non-null   float64
     1   volatile acidity      1599 non-null   float64
     2   citric acid           1599 non-null   float64
     3   residual sugar        1599 non-null   float64
     4   chlorides             1599 non-null   float64
     5   free sulfur dioxide   1599 non-null   float64
     6   total sulfur dioxide  1599 non-null   float64
     7   density               1599 non-null   float64
     8   pH                    1599 non-null   float64
     9   sulphates             1599 non-null   float64
     10  alcohol               1599 non-null   float64
     11  quality               1599 non-null   int64  
    dtypes: float64(11), int64(1)
    memory usage: 150.0 KB


## Data exploration and cleaning

This data is relatively clean to begin. By calling head and info on the dataframe, we get a glimpse at the features and type. There are 12 features; easily indentifiable as 11 independent and 1 dependent variable. They are all numeric random variables. There are not any NULL values. 

The 11 features may or may not have an impact on the quality of the wine. As I am not a wine expert, I have little intuition about the relationship that these features have on quality or the relations that they share between each other. A pairplot will give me insight to answer this question as well as give histograms of each distribution. 


```python
import seaborn as sns
p = sns.pairplot(df)
```

<img src="{{ site.url }}{{ site.baseurl }}/images/pf1.png" alt="">



The pairplot is vast and requires more analysis, but I do get some basic information from it. 

Many variables have a distribution that is right skewed. I will need to transform these variables. I also begin to notice the indivdual impacts that the features have on the quality of the wine. This is more difficult than usual due to the discrete scale of the quality and the continuous scale of the features (this is why the banding occurs). Still, one can recognize the negative relationship that volatile acidity has on quality and the positive relationship that alcohol has on quality. Also noted is the strong positive relationship between fixed acidity and density and a strong negative relationship between fixed acidity and pH. 

Some of this is intuitive - as the acidity increases, the pH decreases. As the free sulfur dioxide increases, so too does the total sulfur dioxide. Because of these relationships that seem to be linear combinations of each other, I am concerned with multicolinearity. In practice, I would research these terms to a greater degree and make a decision whether or not to omit a feature or two. Let's see what that looks like.


```python
plt.scatter(df["fixed acidity"], df["pH"])
plt.xlabel("Fixed Acidity")
plt.ylabel("pH")
plt.title("Fixed Acidity vs pH")
```




    Text(0.5, 1.0, 'Fixed Acidity vs pH')




<img src="{{ site.url }}{{ site.baseurl }}/images/pf2.png" alt="">



```python
 np.corrcoef(df["fixed acidity"], df["pH"])
```




    array([[ 1.        , -0.68297819],
           [-0.68297819,  1.        ]])



Because there is a strong negative correlation between the features, I will omit pH. In a perfect world I would be more familiar with the variables and have an understanding of the science between the acidity variables and the pH. Here I have concluded that it is too closely related and the negatives of multicolinearity outweigh the possible detriment to the model of omitted variable bias. In other words, I have concluded that any explanatory power that pH has is already captured in the acidity variables. 


```python
df = df.drop(['pH'], axis=1)
```

Now, I wish to explore the relationship between free sulfur dioxide and total sulfur dioxide. My hunch is that total sulfur dioxide is the sum of free sulfur dioxide plus additional sulfur dioxide. 


```python
plt.scatter(df["free sulfur dioxide"], df["total sulfur dioxide"])
plt.plot(df["free sulfur dioxide"],df["free sulfur dioxide"], color = "orange")
plt.xlabel("Free")
plt.ylabel("Total")
plt.title("Free Sulfur Dioxide vs Total Sulfur Dioxide")

```




    Text(0.5, 1.0, 'Free Sulfur Dioxide vs Total Sulfur Dioxide')




<img src="{{ site.url }}{{ site.baseurl }}/images/pf3.png" alt="">


The chart above confirms my hunch that total sulfur dioxide is a sum of free sulfur dioxide and additional sulfur dioxide. Because these deltas are not constant, I will create a new variable without this additive relation. 


```python
df["additional sulfur dioxide"] = df["total sulfur dioxide"] - df["free sulfur dioxide"]
df = df.drop(['total sulfur dioxide'], axis=1)

```

Now I will transform the variables that had a skew right distribution. This is to meet the assumption of normality for all features in linear regression to have unbiased estimator coefficients. 


```python
df["fixed acidity"] = np.log(df["fixed acidity"])
df["volatile acidity"] = np.log(df["volatile acidity"])
df["citric acid"] = np.log(df["citric acid"]+ 1)
df["residual sugar"] = np.log(df["residual sugar"])
df["chlorides"] = np.log(df["chlorides"])
df["free sulfur dioxide"] = np.log(df["free sulfur dioxide"])
df["additional sulfur dioxide"] = np.log(df["additional sulfur dioxide"])
df["sulphates"] = np.log(df["sulphates"])
df["alcohol"] = np.log(df["alcohol"])

#It is wise to take a peek at these distibution that result from the tranformations
#This is an example of one histogram of a transformed feature

h = plt.hist(df["free sulfur dioxide"])
# This histogram tell us the transformation is approximately normal
       
```


<img src="{{ site.url }}{{ site.baseurl }}/images/pf4.png" alt="">


## Modeling

It is now time to model. The goal is to predict the quality of the wine given the features. We will use the cleaned data for the modeling. After orienting myself with the data again, I'll split the data into the predictive features and the dependent variable. 


```python
df.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>fixed acidity</th>
      <th>volatile acidity</th>
      <th>citric acid</th>
      <th>residual sugar</th>
      <th>chlorides</th>
      <th>free sulfur dioxide</th>
      <th>density</th>
      <th>sulphates</th>
      <th>alcohol</th>
      <th>quality</th>
      <th>additional sulfur dioxide</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2.001480</td>
      <td>-0.356675</td>
      <td>0.000000</td>
      <td>0.641854</td>
      <td>-2.577022</td>
      <td>2.397895</td>
      <td>0.9978</td>
      <td>-0.579818</td>
      <td>2.240710</td>
      <td>5</td>
      <td>3.135494</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2.054124</td>
      <td>-0.127833</td>
      <td>0.000000</td>
      <td>0.955511</td>
      <td>-2.322788</td>
      <td>3.218876</td>
      <td>0.9968</td>
      <td>-0.385662</td>
      <td>2.282382</td>
      <td>5</td>
      <td>3.737670</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2.054124</td>
      <td>-0.274437</td>
      <td>0.039221</td>
      <td>0.832909</td>
      <td>-2.385967</td>
      <td>2.708050</td>
      <td>0.9970</td>
      <td>-0.430783</td>
      <td>2.282382</td>
      <td>5</td>
      <td>3.663562</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2.415914</td>
      <td>-1.272966</td>
      <td>0.444686</td>
      <td>0.641854</td>
      <td>-2.590267</td>
      <td>2.833213</td>
      <td>0.9980</td>
      <td>-0.544727</td>
      <td>2.282382</td>
      <td>6</td>
      <td>3.761200</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2.001480</td>
      <td>-0.356675</td>
      <td>0.000000</td>
      <td>0.641854</td>
      <td>-2.577022</td>
      <td>2.397895</td>
      <td>0.9978</td>
      <td>-0.579818</td>
      <td>2.240710</td>
      <td>5</td>
      <td>3.135494</td>
    </tr>
  </tbody>
</table>
</div>




```python
# This is just to rearrange the columns to make slicing the data easier. 
cols = df.columns.tolist()
cols
```




    ['fixed acidity',
     'volatile acidity',
     'citric acid',
     'residual sugar',
     'chlorides',
     'free sulfur dioxide',
     'density',
     'sulphates',
     'alcohol',
     'quality',
     'additional sulfur dioxide']




```python
cols = ['fixed acidity','volatile acidity','citric acid','residual sugar','chlorides','free sulfur dioxide','density',
        'sulphates','alcohol','additional sulfur dioxide','quality']

df = df[cols] 
```


```python
X = df.iloc[:, 0:10].values
y = df.iloc[:, 10].values
```

X and y features are well created. Now I will split the data into a test and train set.


```python
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)

```

To this point, I've successfully created our X and y features and split them into two groups, one for training the model and one for testing the model. I set the test size to .2, meaning that 80% of the data will be used to train the model and the other 20% to test and score the model. I will begin with a model of degree 1. This is the same as a multiple linear regression. 


```python
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
polynomial_reg = PolynomialFeatures(degree = 1)
X_poly = polynomial_reg.fit_transform(X_train)
polynomial_reg.fit(X_poly, y_train)
linear_reg = LinearRegression()
linear_reg.fit(X_poly, y_train)
```




    LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)




```python
predictions_reg_1 = linear_reg.predict(polynomial_reg.fit_transform(X_test))
```


```python
accuracy_reg_1 = np.absolute(np.around(predictions_reg_1) - y_test)
plt.hist(accuracy_reg_1)
plt.show()
```


<img src="{{ site.url }}{{ site.baseurl }}/images/pf5.png" alt="">



```python
accuracy_metric = np.mean(accuracy_reg_1)
print(1-accuracy_metric)
```

    0.56875


This model does okay to rank the quality of the wine. We see that we must be creative in how to measure success. Mindful that we want to compare models, the accuracy metric must be something that can be compared. Here, I subtracted the test values from the rounded predictions to categorize the number of correct predictions and also how "off" the predictions were with the wines it did not rank correctly. The histogram provides some insights. Then I used 1 minus the mean of all values from this calucation to score the model. It is generic, but it will serve the purpose for which it is needed.  

On to models with features of degree 1 and 2. A quick reminder that this is also linear regression because the coefficients are linear. The non-linear portion is limited to the features.


```python
polynomial_reg_2 = PolynomialFeatures(degree = 2)
X_poly_2 = polynomial_reg_2.fit_transform(X_train)
polynomial_reg_2.fit(X_poly_2, y_train)
linear_reg_2 = LinearRegression()
linear_reg_2.fit(X_poly_2, y_train)
```




    LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)




```python
predictions_reg_2 = linear_reg_2.predict(polynomial_reg_2.fit_transform(X_test))
```


```python
accuracy_reg_2 = np.absolute(np.around(predictions_reg_2) - y_test)
plt.hist(accuracy_reg_2)
plt.show()
```


<img src="{{ site.url }}{{ site.baseurl }}/images/pf6.png" alt="">


```python
accuracy_metric_2 = np.mean(accuracy_reg_2)
print(1-accuracy_metric_2)
```

    0.584375


While this model correctly predicted about 200 wines out of 320 and the accuracy metric is greater than the model of degree 1, the added complexity is barely worth it and did not change the model significantly. 

It is worth pointing out that this problem's approach is similar to that of a hypothesis, although I am certainly not equating the two. In other words, there must be evidence above a certain threshold to reject the simpler method and choose the more complex method. However, the objective was to play with the models and explore. I'll continue on that road.

Lastly, let's try a model with x, x^2, and x^3. 


```python
polynomial_reg_3 = PolynomialFeatures(degree = 3)
X_poly_3 = polynomial_reg_3.fit_transform(X_train)
polynomial_reg_3.fit(X_poly_3, y_train)
linear_reg_3 = LinearRegression()
linear_reg_3.fit(X_poly_3, y_train)
```




    LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)




```python
predictions_reg_3 = linear_reg_3.predict(polynomial_reg_3.fit_transform(X_test))
```


```python
accuracy_reg_3 = np.absolute(np.around(predictions_reg_3) - y_test)
plt.hist(accuracy_reg_3)
plt.show()
```


<img src="{{ site.url }}{{ site.baseurl }}/images/pf7.png" alt="">



```python
accuracy_metric_3 = np.mean(accuracy_reg_3)
print(1-accuracy_metric_3)
```

    0.54375


This model is worse than model 2 and model 1 in regards to the accuracy metric. The complexity of adding X^3 terms decreases the model's effectiveness.

## Conclusion

After exploring with polynomial features, we can conclude that a multiple linear regression is a fine regression model to predict quality of wine with the available features.

It was difficult to work with regression due to the discrete scale that the dependent variable had. This dataset lends itself to many machine learning algorithms, but I thought that this was a fun application. In this project, I found that adding polynomial features to the model did not significantly improve the performance of the models. This is not always true and should be explored from time to time to see if you can improve a given model. In lower dimensions, simply plotting the variables can prove useful in determining the degree of the regression, but obviously with the features we had to work with here, that is not helpful. 


