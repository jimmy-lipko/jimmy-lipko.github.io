---
title: "Deep Neural Networks: Forecasting Churn"
date: 2020-04-04
tags: [Deep Learning]
header: 
    image: "/images/neuron.jpg"
excerpt: "Telecom Customer Churn Prediction"
---


In this project, I want to utalize an artificial neural network to
predict the churn of a Telecom company. The churn models presented can
be applied to other fields such as banking, social networks, or any
other consumer facing business. The goal of the algorithm is to predict
how likely it is that a current customer will discontinue their
membership or use soon. This will allow the firm to have more accurate
forecasting in terms of revenue, working capital, labor, etc.

The consumers with the highest likihood to leave and those that have the
highest detriment to the firm if they did leave can be selectively
targeted by the marketing or customer care department.

I will also attempt to explain the mathematics behind the algorithms and
link to other articles that I think provide greater insight into how and
why the algorithms are effective.

### Load the Data


Let’s jump right in by loading the data:

    # Data can be read from a local csv file or queried from a database. 
    df <- read.csv("dataset_4_4.csv")
    head(df)

    ##   customerID gender SeniorCitizen Partner Dependents tenure PhoneService
    ## 1 7590-VHVEG Female             0     Yes         No      1           No
    ## 2 5575-GNVDE   Male             0      No         No     34          Yes
    ## 3 3668-QPYBK   Male             0      No         No      2          Yes
    ## 4 7795-CFOCW   Male             0      No         No     45           No
    ## 5 9237-HQITU Female             0      No         No      2          Yes
    ## 6 9305-CDSKC Female             0      No         No      8          Yes
    ##      MultipleLines InternetService OnlineSecurity OnlineBackup DeviceProtection
    ## 1 No phone service             DSL             No          Yes               No
    ## 2               No             DSL            Yes           No              Yes
    ## 3               No             DSL            Yes          Yes               No
    ## 4 No phone service             DSL            Yes           No              Yes
    ## 5               No     Fiber optic             No           No               No
    ## 6              Yes     Fiber optic             No           No              Yes
    ##   TechSupport StreamingTV StreamingMovies       Contract PaperlessBilling
    ## 1          No          No              No Month-to-month              Yes
    ## 2          No          No              No       One year               No
    ## 3          No          No              No Month-to-month              Yes
    ## 4         Yes          No              No       One year               No
    ## 5          No          No              No Month-to-month              Yes
    ## 6          No         Yes             Yes Month-to-month              Yes
    ##               PaymentMethod MonthlyCharges TotalCharges Churn
    ## 1          Electronic check          29.85        29.85    No
    ## 2              Mailed check          56.95      1889.50    No
    ## 3              Mailed check          53.85       108.15   Yes
    ## 4 Bank transfer (automatic)          42.30      1840.75    No
    ## 5          Electronic check          70.70       151.65   Yes
    ## 6          Electronic check          99.65       820.50   Yes

Immediately, we notice that this is a very wide data set. This is good
because we have a lot of features that can explain whether or not a
customer leaves. This might also create some challenges with
multicolinearity or overfitting. To avoid this, we will use the “Rule of
Ten” as a guide more than a rule. For more information on this rule,
check [this](https://en.wikipedia.org/wiki/One_in_ten_rule) out!

Clean the Data
--------------

With all of these variables, we have a lot of Data Cleaning to work
through. We can call structure on the dataframe to examine each variable
on an individual basis.

    str(df)

    ## 'data.frame':    7043 obs. of  21 variables:
    ##  $ customerID      : Factor w/ 7043 levels "0002-ORFBO","0003-MKNFE",..: 5376 3963 2565 5536 6512 6552 1003 4771 5605 4535 ...
    ##  $ gender          : Factor w/ 2 levels "Female","Male": 1 2 2 2 1 1 2 1 1 2 ...
    ##  $ SeniorCitizen   : int  0 0 0 0 0 0 0 0 0 0 ...
    ##  $ Partner         : Factor w/ 2 levels "No","Yes": 2 1 1 1 1 1 1 1 2 1 ...
    ##  $ Dependents      : Factor w/ 2 levels "No","Yes": 1 1 1 1 1 1 2 1 1 2 ...
    ##  $ tenure          : int  1 34 2 45 2 8 22 10 28 62 ...
    ##  $ PhoneService    : Factor w/ 2 levels "No","Yes": 1 2 2 1 2 2 2 1 2 2 ...
    ##  $ MultipleLines   : Factor w/ 3 levels "No","No phone service",..: 2 1 1 2 1 3 3 2 3 1 ...
    ##  $ InternetService : Factor w/ 3 levels "DSL","Fiber optic",..: 1 1 1 1 2 2 2 1 2 1 ...
    ##  $ OnlineSecurity  : Factor w/ 3 levels "No","No internet service",..: 1 3 3 3 1 1 1 3 1 3 ...
    ##  $ OnlineBackup    : Factor w/ 3 levels "No","No internet service",..: 3 1 3 1 1 1 3 1 1 3 ...
    ##  $ DeviceProtection: Factor w/ 3 levels "No","No internet service",..: 1 3 1 3 1 3 1 1 3 1 ...
    ##  $ TechSupport     : Factor w/ 3 levels "No","No internet service",..: 1 1 1 3 1 1 1 1 3 1 ...
    ##  $ StreamingTV     : Factor w/ 3 levels "No","No internet service",..: 1 1 1 1 1 3 3 1 3 1 ...
    ##  $ StreamingMovies : Factor w/ 3 levels "No","No internet service",..: 1 1 1 1 1 3 1 1 3 1 ...
    ##  $ Contract        : Factor w/ 3 levels "Month-to-month",..: 1 2 1 2 1 1 1 1 1 2 ...
    ##  $ PaperlessBilling: Factor w/ 2 levels "No","Yes": 2 1 2 1 2 2 2 1 2 1 ...
    ##  $ PaymentMethod   : Factor w/ 4 levels "Bank transfer (automatic)",..: 3 4 4 1 3 3 2 4 3 1 ...
    ##  $ MonthlyCharges  : num  29.9 57 53.9 42.3 70.7 ...
    ##  $ TotalCharges    : num  29.9 1889.5 108.2 1840.8 151.7 ...
    ##  $ Churn           : Factor w/ 2 levels "No","Yes": 1 1 2 1 2 2 1 1 2 1 ...

    summary(is.na(df))

    ##  customerID        gender        SeniorCitizen    Partner       
    ##  Mode :logical   Mode :logical   Mode :logical   Mode :logical  
    ##  FALSE:7043      FALSE:7043      FALSE:7043      FALSE:7043     
    ##                                                                 
    ##  Dependents        tenure        PhoneService    MultipleLines  
    ##  Mode :logical   Mode :logical   Mode :logical   Mode :logical  
    ##  FALSE:7043      FALSE:7043      FALSE:7043      FALSE:7043     
    ##                                                                 
    ##  InternetService OnlineSecurity  OnlineBackup    DeviceProtection
    ##  Mode :logical   Mode :logical   Mode :logical   Mode :logical   
    ##  FALSE:7043      FALSE:7043      FALSE:7043      FALSE:7043      
    ##                                                                  
    ##  TechSupport     StreamingTV     StreamingMovies  Contract      
    ##  Mode :logical   Mode :logical   Mode :logical   Mode :logical  
    ##  FALSE:7043      FALSE:7043      FALSE:7043      FALSE:7043     
    ##                                                                 
    ##  PaperlessBilling PaymentMethod   MonthlyCharges  TotalCharges   
    ##  Mode :logical    Mode :logical   Mode :logical   Mode :logical  
    ##  FALSE:7043       FALSE:7043      FALSE:7043      FALSE:7032     
    ##                                                   TRUE :11       
    ##    Churn        
    ##  Mode :logical  
    ##  FALSE:7043     
    ## 

First, we learn that we have 7,043 observations and 21 different variables. We all note 11 NA values, and since it is only 11 we omit.

- Customer ID was most likely a primary key in the database where this data was housed. We can omit it. 

- Gender is a factored variable - Male/Female, we will make this a numberic factor.  

- Senior Citizen is a int, we will make this a numeric factor. Partner is a factored variable Yes/No, we will make this a numberic factor.  

- Dependents is a factored variable Yes/No, we will make this a numberic factor.  

- Tenure is a continuous integer variable, no modification neccessary

- Phone Service is a factored variable Yes/No, we will make this a
numberic factor.  

- Multiple lines is a factored variable that is depedent on Phone service, it has too much of a relationship with Phone service. We can omit this varaible.

- Internet Service is a factored variable with 3 levels, we will make this a numberic factor.

- Online Security, Online Backup, Device Protection, Tech Support, Streaming TV, and Steaming Movies is all dependent on Internet Service and will be omitted.  

- Contract is a factored variable with 3 levels, we will make this a numberic factor.

- Paperless Billing is a factored variable Yes/No, we will make this a numberic factor. 

- Payment Method is a factored variable with 4 levels, we will make this a numberic factor.

- Montlhy Charges is a continuous numeric variable, no changes neccessary 

- Total Payment is a continuous numeric variable, not sure the relationship with Monthly Charges - may omit if linearly dependent

- Churn is a factored variable Yes/No, we will make this a numberic factor. This is our dependent variable.

    df <- na.omit(df)
    df <- df[,-c(1,8,10:15)]
    df$gender <- as.numeric(factor(df$gender, levels = c("Female", "Male"), labels = c(1:2)))
    df$SeniorCitizen <- as.numeric(factor(df$SeniorCitizen, levels = c(0,1), labels = c(1:2)))
    df$Partner <- as.numeric(factor(df$Partner, levels = c("No","Yes"), labels = c(1:2)))
    df$Dependents<- as.numeric(factor(df$Dependents, levels = c("No","Yes"), labels = c(1:2)))
    df$PhoneService <- as.numeric(factor(df$PhoneService, levels = c("No","Yes"), labels = c(1:2)))
    df$InternetService <- as.numeric(factor(df$InternetService, levels = c("No","DSL","Fiber optic"), labels = c(1:3)))
    df$Contract <- as.numeric(factor(df$Contract, levels = c("Month-to-month", "One year","Two year"), labels = c(1:3)))
    df$PaperlessBilling <- as.numeric(factor(df$PaperlessBilling, levels = c("No","Yes"), labels = c(1:2)))
    df$PaymentMethod <- as.numeric(factor(df$PaymentMethod, levels = c("Electronic check","Mailed check","Bank transfer (automatic)","Credit card (automatic)"), labels = c(1:4)))
    df$Churn <- as.numeric(factor(df$Churn, levels = c("No","Yes"), labels = c(1,2)))

    str(df)

    ## 'data.frame':    7032 obs. of  13 variables:
    ##  $ gender          : num  1 2 2 2 1 1 2 1 1 2 ...
    ##  $ SeniorCitizen   : num  1 1 1 1 1 1 1 1 1 1 ...
    ##  $ Partner         : num  2 1 1 1 1 1 1 1 2 1 ...
    ##  $ Dependents      : num  1 1 1 1 1 1 2 1 1 2 ...
    ##  $ tenure          : int  1 34 2 45 2 8 22 10 28 62 ...
    ##  $ PhoneService    : num  1 2 2 1 2 2 2 1 2 2 ...
    ##  $ InternetService : num  2 2 2 2 3 3 3 2 3 2 ...
    ##  $ Contract        : num  1 2 1 2 1 1 1 1 1 2 ...
    ##  $ PaperlessBilling: num  2 1 2 1 2 2 2 1 2 1 ...
    ##  $ PaymentMethod   : num  1 2 2 3 1 1 4 2 1 3 ...
    ##  $ MonthlyCharges  : num  29.9 57 53.9 42.3 70.7 ...
    ##  $ TotalCharges    : num  29.9 1889.5 108.2 1840.8 151.7 ...
    ##  $ Churn           : num  1 1 2 1 2 2 1 1 2 1 ...

Lastly, lets check up on the payment relationship:

    plot(df$MonthlyCharges, df$TotalCharges)

img src="{{ site.url }}{{ site.baseurl }}/images/plot 1 nn.jpg" alt="">

It seems that the charges are dependent on each other. I am going to
omit the monthly charges and use only the total charges as a feature.
The frequency at which the bill is paid is already factored into the
model via the Contract variable.

    df <- df[,-11]
    str(df)

    ## 'data.frame':    7032 obs. of  12 variables:
    ##  $ gender          : num  1 2 2 2 1 1 2 1 1 2 ...
    ##  $ SeniorCitizen   : num  1 1 1 1 1 1 1 1 1 1 ...
    ##  $ Partner         : num  2 1 1 1 1 1 1 1 2 1 ...
    ##  $ Dependents      : num  1 1 1 1 1 1 2 1 1 2 ...
    ##  $ tenure          : int  1 34 2 45 2 8 22 10 28 62 ...
    ##  $ PhoneService    : num  1 2 2 1 2 2 2 1 2 2 ...
    ##  $ InternetService : num  2 2 2 2 3 3 3 2 3 2 ...
    ##  $ Contract        : num  1 2 1 2 1 1 1 1 1 2 ...
    ##  $ PaperlessBilling: num  2 1 2 1 2 2 2 1 2 1 ...
    ##  $ PaymentMethod   : num  1 2 2 3 1 1 4 2 1 3 ...
    ##  $ TotalCharges    : num  29.9 1889.5 108.2 1840.8 151.7 ...
    ##  $ Churn           : num  1 1 2 1 2 2 1 1 2 1 ...

    library(dplyr)

    ## 
    ## Attaching package: 'dplyr'

    ## The following objects are masked from 'package:stats':
    ## 
    ##     filter, lag

    ## The following objects are masked from 'package:base':
    ## 
    ##     intersect, setdiff, setequal, union

    df %>% group_by(Churn) %>% tally

    ## # A tibble: 2 x 2
    ##   Churn     n
    ##   <dbl> <int>
    ## 1     1  5163
    ## 2     2  1869

Data cleaning is now finished. We can use all variables according to the
rule of ten, in fact we could have used up to 18. For the purposes of
this post, I want to explore the models in great detail. To avoid making
the post too long, I will skip data exploration. This is a very
important and not a trivial step. I would encourage you to perform your
own on this data set. Some exploration might be to see boxplots of the
bill amounts by demographics. Can follow this up with anova testing too!

### Modeling

We wish to explore both logistic regression and neural net modeling.

    library(keras)
    df_nn <- df
    df_nn <- as.matrix(df_nn)
    dimnames(df_nn) <- NULL

    df_nn[, 1] <- normalize(df_nn[,1])
    df_nn[, 2] <- normalize(df_nn[,2])
    df_nn[, 3] <- normalize(df_nn[,3])
    df_nn[, 4] <- normalize(df_nn[,4])
    df_nn[, 5] <- normalize(df_nn[,5])
    df_nn[, 6] <- normalize(df_nn[,6])
    df_nn[, 7] <- normalize(df_nn[,7])
    df_nn[, 8] <- normalize(df_nn[,8])
    df_nn[, 9] <- normalize(df_nn[,9])
    df_nn[, 10] <- normalize(df_nn[,10])
    df_nn[, 11] <- normalize(df_nn[,11])
    df_nn[,12] <- df_nn[,12] - 1
    summary(df_nn)

    ##        V1                 V2                 V3                 V4          
    ##  Min.   :0.007521   Min.   :0.009779   Min.   :0.007622   Min.   :0.008662  
    ##  1st Qu.:0.007521   1st Qu.:0.009779   1st Qu.:0.007622   1st Qu.:0.008662  
    ##  Median :0.015042   Median :0.009779   Median :0.007622   Median :0.008662  
    ##  Mean   :0.011317   Mean   :0.011367   Mean   :0.011300   Mean   :0.011247  
    ##  3rd Qu.:0.015042   3rd Qu.:0.009779   3rd Qu.:0.015245   3rd Qu.:0.017323  
    ##  Max.   :0.015042   Max.   :0.019557   Max.   :0.015245   Max.   :0.017323  
    ##        V5                  V6                 V7                 V8          
    ##  Min.   :0.0002933   Min.   :0.006191   Min.   :0.005061   Min.   :0.006334  
    ##  1st Qu.:0.0026393   1st Qu.:0.012383   1st Qu.:0.010121   1st Qu.:0.006334  
    ##  Median :0.0085045   Median :0.012383   Median :0.010121   Median :0.006334  
    ##  Mean   :0.0095080   Mean   :0.011784   Mean   :0.011255   Mean   :0.010695  
    ##  3rd Qu.:0.0161292   3rd Qu.:0.012383   3rd Qu.:0.015182   3rd Qu.:0.012667  
    ##  Max.   :0.0211146   Max.   :0.012383   Max.   :0.015182   Max.   :0.019001  
    ##        V9                V10                V11                 V12        
    ##  Min.   :0.007155   Min.   :0.004613   Min.   :6.968e-05   Min.   :0.0000  
    ##  1st Qu.:0.007155   1st Qu.:0.004613   1st Qu.:1.488e-03   1st Qu.:0.0000  
    ##  Median :0.014309   Median :0.009226   Median :5.180e-03   Median :0.0000  
    ##  Mean   :0.011395   Mean   :0.010681   Mean   :8.463e-03   Mean   :0.2658  
    ##  3rd Qu.:0.014309   3rd Qu.:0.013839   3rd Qu.:1.407e-02   3rd Qu.:1.0000  
    ##  Max.   :0.014309   Max.   :0.018452   Max.   :3.219e-02   Max.   :1.0000

In this bit of code above, I am formatting the data for the neural
network. The independent variables have all been normalized and the
dependent variable is either 0 or 1.

Now we split this data into a test set and a training set.

    #Splitting Dataset into test and train
    library(caTools)
    set.seed(123)
    ind <- sample(2, nrow(df_nn), replace = T, prob = c(.8, .2))
    train <- df_nn[ind==1, 1:11]
    test <- df_nn[ind==2, 1:11]
    train_y<- df_nn[ind==1, 12]
    test_y <- df_nn[ind==2, 12]

Now we are ready to build the neural network. I will start with two
hidden layers and an output layer. This should be sufficient to maximize
the predictive power without overtraining.

    nn <- keras_model_sequential()

    nn %>%
      layer_dense(units=256, activation = "relu", input_shape = 11) %>%
      layer_dropout(rate = .25) %>%
      layer_dense(units=128, activation = "relu") %>%
      layer_dropout(rate = .10) %>%
      layer_dense(units = 1, activation = "sigmoid") %>%
        compile(loss = 'binary_crossentropy',
              optimizer = optimizer_adam(lr= 0.01, decay = 1e-6), metrics = c('accuracy'))

Now we fit the model to our data.

    model <- nn %>%
             fit(train,
                 train_y,
                 epoch = 50,
                 batch_size = 128,
                 validation_split = 0.25)
    plot(model)

    ## `geom_smooth()` using formula 'y ~ x'

img src="{{ site.url }}{{ site.baseurl }}/images/plot 2 nn.jpg" alt="">

We have run and fit our model. You will notice that our validation and
our training accuracy run pretty tight. This is a good indication that e
did not overfit the model. Let us use this model to predict the churn of
our test set.

    testingmodel <- nn %>%
             evaluate(test, test_y)
    print(as.data.frame(testingmodel))

    ##        loss  accuracy
    ## 1 0.4434731 0.7936736

    pred <- nn %>%
             predict_classes(test)
    cm <- table(Predicted = pred, Actual = test_y)
    print(cm)

    ##          Actual
    ## Predicted   0   1
    ##         0 927 200
    ##         1  87 177

This confusion matrix shows that we would have accurately predicted that
193 customers would leave our service. If a marketing campaign were
tasked with reaching out to a specific group, they would have reached
264 people, 177 of which would have left and the other 87 would have
stayed regarless. They would have missed 200 people. We correctly
identified 927 customers who did not discountinue services.