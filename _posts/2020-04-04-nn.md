---
title: "Deep Neural Networks: Forecasting Churn"
date: 2020-04-04
tags: [Deep Learning]
header: 
    image: "/images/neuron.jpg"
excerpt: "Telecom Customer Churn Prediction"
---

In this project, I want to utilize an artificial neural network to
predict the churn of a Telecom company. The churn model presented can be
applied to other fields such as banking, social networks, or any other
consumer facing business. The objective of this algorithm is to predict
how likely it is that a current customer will discontinue their
membership or their product use soon. This will allow the firm to have
more accurate forecasting in terms of revenue, working capital, labor,
etc.

Applications of the predictions from the algorithm are numberous. One
such example might be selective targeting by the marketing or customer
care department.

I will also attempt to explain some of the mathematics behind the
algorithms and link to other articles that I think provide greater
insight into how and why the algorithms are effective.

Load the Data
-------------

Letâ€™s jump right in by loading the data:

    # Data can be read from a local csv file or queried from a database. 
    df <- read.csv("dataset_4_4.csv")
    head(df)

    ##   customerID gender SeniorCitizen Partner Dependents tenure PhoneService
    ## 1 7590-VHVEG Female             0     Yes         No      1           No
    ## 2 5575-GNVDE   Male             0      No         No     34          Yes
    ## 3 3668-QPYBK   Male             0      No         No      2          Yes
    ## 4 7795-CFOCW   Male             0      No         No     45           No
    ## 5 9237-HQITU Female             0      No         No      2          Yes
    ## 6 9305-CDSKC Female             0      No         No      8          Yes
    ##      MultipleLines InternetService OnlineSecurity OnlineBackup DeviceProtection
    ## 1 No phone service             DSL             No          Yes               No
    ## 2               No             DSL            Yes           No              Yes
    ## 3               No             DSL            Yes          Yes               No
    ## 4 No phone service             DSL            Yes           No              Yes
    ## 5               No     Fiber optic             No           No               No
    ## 6              Yes     Fiber optic             No           No              Yes
    ##   TechSupport StreamingTV StreamingMovies       Contract PaperlessBilling
    ## 1          No          No              No Month-to-month              Yes
    ## 2          No          No              No       One year               No
    ## 3          No          No              No Month-to-month              Yes
    ## 4         Yes          No              No       One year               No
    ## 5          No          No              No Month-to-month              Yes
    ## 6          No         Yes             Yes Month-to-month              Yes
    ##               PaymentMethod MonthlyCharges TotalCharges Churn
    ## 1          Electronic check          29.85        29.85    No
    ## 2              Mailed check          56.95      1889.50    No
    ## 3              Mailed check          53.85       108.15   Yes
    ## 4 Bank transfer (automatic)          42.30      1840.75    No
    ## 5          Electronic check          70.70       151.65   Yes
    ## 6          Electronic check          99.65       820.50   Yes

Immediately, we notice that this is a very wide data set. This is good
because we have a lot of features that can explain whether or not a
customer leaves. This might also create some challenges with
multicolinearity or overfitting. These issues might be a greater problem
in a linear or a logistic regression.

Clean the Data
--------------

With these variables, we have a lot of Data Cleaning to work through. We
can call structure on the dataframe to examine each variable on an
individual basis.

    str(df)

    ## 'data.frame':    7043 obs. of  21 variables:
    ##  $ customerID      : Factor w/ 7043 levels "0002-ORFBO","0003-MKNFE",..: 5376 3963 2565 5536 6512 6552 1003 4771 5605 4535 ...
    ##  $ gender          : Factor w/ 2 levels "Female","Male": 1 2 2 2 1 1 2 1 1 2 ...
    ##  $ SeniorCitizen   : int  0 0 0 0 0 0 0 0 0 0 ...
    ##  $ Partner         : Factor w/ 2 levels "No","Yes": 2 1 1 1 1 1 1 1 2 1 ...
    ##  $ Dependents      : Factor w/ 2 levels "No","Yes": 1 1 1 1 1 1 2 1 1 2 ...
    ##  $ tenure          : int  1 34 2 45 2 8 22 10 28 62 ...
    ##  $ PhoneService    : Factor w/ 2 levels "No","Yes": 1 2 2 1 2 2 2 1 2 2 ...
    ##  $ MultipleLines   : Factor w/ 3 levels "No","No phone service",..: 2 1 1 2 1 3 3 2 3 1 ...
    ##  $ InternetService : Factor w/ 3 levels "DSL","Fiber optic",..: 1 1 1 1 2 2 2 1 2 1 ...
    ##  $ OnlineSecurity  : Factor w/ 3 levels "No","No internet service",..: 1 3 3 3 1 1 1 3 1 3 ...
    ##  $ OnlineBackup    : Factor w/ 3 levels "No","No internet service",..: 3 1 3 1 1 1 3 1 1 3 ...
    ##  $ DeviceProtection: Factor w/ 3 levels "No","No internet service",..: 1 3 1 3 1 3 1 1 3 1 ...
    ##  $ TechSupport     : Factor w/ 3 levels "No","No internet service",..: 1 1 1 3 1 1 1 1 3 1 ...
    ##  $ StreamingTV     : Factor w/ 3 levels "No","No internet service",..: 1 1 1 1 1 3 3 1 3 1 ...
    ##  $ StreamingMovies : Factor w/ 3 levels "No","No internet service",..: 1 1 1 1 1 3 1 1 3 1 ...
    ##  $ Contract        : Factor w/ 3 levels "Month-to-month",..: 1 2 1 2 1 1 1 1 1 2 ...
    ##  $ PaperlessBilling: Factor w/ 2 levels "No","Yes": 2 1 2 1 2 2 2 1 2 1 ...
    ##  $ PaymentMethod   : Factor w/ 4 levels "Bank transfer (automatic)",..: 3 4 4 1 3 3 2 4 3 1 ...
    ##  $ MonthlyCharges  : num  29.9 57 53.9 42.3 70.7 ...
    ##  $ TotalCharges    : num  29.9 1889.5 108.2 1840.8 151.7 ...
    ##  $ Churn           : Factor w/ 2 levels "No","Yes": 1 1 2 1 2 2 1 1 2 1 ...

    summary(is.na(df))

    ##  customerID        gender        SeniorCitizen    Partner       
    ##  Mode :logical   Mode :logical   Mode :logical   Mode :logical  
    ##  FALSE:7043      FALSE:7043      FALSE:7043      FALSE:7043     
    ##                                                                 
    ##  Dependents        tenure        PhoneService    MultipleLines  
    ##  Mode :logical   Mode :logical   Mode :logical   Mode :logical  
    ##  FALSE:7043      FALSE:7043      FALSE:7043      FALSE:7043     
    ##                                                                 
    ##  InternetService OnlineSecurity  OnlineBackup    DeviceProtection
    ##  Mode :logical   Mode :logical   Mode :logical   Mode :logical   
    ##  FALSE:7043      FALSE:7043      FALSE:7043      FALSE:7043      
    ##                                                                  
    ##  TechSupport     StreamingTV     StreamingMovies  Contract      
    ##  Mode :logical   Mode :logical   Mode :logical   Mode :logical  
    ##  FALSE:7043      FALSE:7043      FALSE:7043      FALSE:7043     
    ##                                                                 
    ##  PaperlessBilling PaymentMethod   MonthlyCharges  TotalCharges   
    ##  Mode :logical    Mode :logical   Mode :logical   Mode :logical  
    ##  FALSE:7043       FALSE:7043      FALSE:7043      FALSE:7032     
    ##                                                   TRUE :11       
    ##    Churn        
    ##  Mode :logical  
    ##  FALSE:7043     
    ## 

First, we learn that we have 7,043 observations and 21 different
variables. We all note 11 NA values, and since it is only 11 we omit.

- Customer ID was most likely a primary key in the database where this data was housed. We can omit it.

- Gender is a factored variable - Male/Female, we will make this a numberic factor.  

- Senior Citizen is a int, we will make this a numeric factor. Partner is a factored variable Yes/No, we will make this a numberic factor.  

- Dependents is a factored variable Yes/No, we will make this a numberic factor.  

- Tenure is a continuous integer variable, no modification neccessary

- Phone Service is a factored variable Yes/No, we will make this a numberic factor.  

- Multiple lines is a factored variable that is depedent on Phone service, it has too much of a relationship with Phone service. We can omit this
varaible.

- Internet Service is a factored variable with 3 levels, we will make this a numberic factor. This will be a bit of a challenge. 

- Online Security, Online Backup, Device Protection, Tech Support, Streaming TV, and Steaming Movies is all dependent on Internet Service and will be omitted. 

- Contract is a factored variable with 3 levels, we will make this a numberic factor. This will be a bit of a challenge. 

- Paperless Billing is a factored variable Yes/No, we will make this a numberic factor. 

- Payment Method is a factored variable with 4 levels, we will make this a numberic factor. This will be a bit of a challenge. 

- Montlhy Charges is a continuous numeric variable, no changes neccessary

- Total Payment is a continuous numeric variable, not sure the relationship with Monthly Charges - may omit if linearly dependent 

- Churn is a factored variable Yes/No, we will make this a numberic factor. This is our
dependent variable.

    library(fastDummies)
    #fastDummies is an excellent library. dummy_cols() takes a dataframe and returns all columns that were factors or characters into dummy variabes. This is the #needed format for this algorithm.
    df_clean <- na.omit(df)
    df_clean <- df_clean[,-c(1,8,10:15)]
    df_clean$SeniorCitizen <- factor(df_clean$SeniorCitizen)
    # We remove the first dummy column to avoid the dummy variable trap.
    df_clean <- dummy_cols(df_clean, remove_first_dummy = TRUE)

    df_clean <- df_clean[,c(5,11,12,14:27)]

    # df$gender <- as.numeric(factor(df$gender, levels = c("Female", "Male"), labels = c(1:2)))
    # df$SeniorCitizen <- as.numeric(factor(df$SeniorCitizen, levels = c(0,1), labels = c(1:2)))
    # df$Partner <- as.numeric(factor(df$Partner, levels = c("No","Yes"), labels = c(1:2)))
    # df$Dependents<- as.numeric(factor(df$Dependents, levels = c("No","Yes"), labels = c(1:2)))
    # df$PhoneService <- as.numeric(factor(df$PhoneService, levels = c("No","Yes"), labels = c(1:2)))
    # InternetService <- as.numeric(factor(df$InternetService, levels = c("No","DSL","Fiber optic"), labels = c(1:3)))
    # Contract <- as.numeric(factor(df$Contract, levels = c("Month-to-month", "One year","Two year"), labels = c(1:3)))
    # df$PaperlessBilling <- as.numeric(factor(df$PaperlessBilling, levels = c("No","Yes"), labels = c(1:2)))
    # PaymentMethod <- as.numeric(factor(df$PaymentMethod, levels = c("Electronic check","Mailed check","Bank transfer (automatic)","Credit card (automatic)"), labels = c(1:4)))
    # df$Churn <- as.numeric(factor(df$Churn, levels = c("No","Yes"), labels = c(1,2)))

    str(df_clean)

    ## 'data.frame':    7032 obs. of  17 variables:
    ##  $ tenure                               : int  1 34 2 45 2 8 22 10 28 62 ...
    ##  $ MonthlyCharges                       : num  29.9 57 53.9 42.3 70.7 ...
    ##  $ TotalCharges                         : num  29.9 1889.5 108.2 1840.8 151.7 ...
    ##  $ gender_Male                          : int  0 1 1 1 0 0 1 0 0 1 ...
    ##  $ SeniorCitizen_1                      : int  0 0 0 0 0 0 0 0 0 0 ...
    ##  $ Partner_Yes                          : int  1 0 0 0 0 0 0 0 1 0 ...
    ##  $ Dependents_Yes                       : int  0 0 0 0 0 0 1 0 0 1 ...
    ##  $ PhoneService_Yes                     : int  0 1 1 0 1 1 1 0 1 1 ...
    ##  $ InternetService_Fiber optic          : int  0 0 0 0 1 1 1 0 1 0 ...
    ##  $ InternetService_No                   : int  0 0 0 0 0 0 0 0 0 0 ...
    ##  $ Contract_One year                    : int  0 1 0 1 0 0 0 0 0 1 ...
    ##  $ Contract_Two year                    : int  0 0 0 0 0 0 0 0 0 0 ...
    ##  $ PaperlessBilling_Yes                 : int  1 0 1 0 1 1 1 0 1 0 ...
    ##  $ PaymentMethod_Credit card (automatic): int  0 0 0 0 0 0 1 0 0 0 ...
    ##  $ PaymentMethod_Electronic check       : int  1 0 0 0 1 1 0 0 1 0 ...
    ##  $ PaymentMethod_Mailed check           : int  0 1 1 0 0 0 0 1 0 0 ...
    ##  $ Churn_Yes                            : int  0 0 1 0 1 1 0 0 1 0 ...

Lastly, lets check up on the payment relationship:

    plot(df_clean$MonthlyCharges, df_clean$TotalCharges)


<img src="{{ site.url }}{{ site.baseurl }}/images/plot 1 nn.jpg" alt="">

It seems that the charges are dependent on each other. I am going to
omit the monthly charges and use only the total charges as a feature.
The frequency at which the bill is paid is already factored into the
model via the Contract variable.

    df_clean <- df_clean[,-2]
    str(df_clean)

    ## 'data.frame':    7032 obs. of  16 variables:
    ##  $ tenure                               : int  1 34 2 45 2 8 22 10 28 62 ...
    ##  $ TotalCharges                         : num  29.9 1889.5 108.2 1840.8 151.7 ...
    ##  $ gender_Male                          : int  0 1 1 1 0 0 1 0 0 1 ...
    ##  $ SeniorCitizen_1                      : int  0 0 0 0 0 0 0 0 0 0 ...
    ##  $ Partner_Yes                          : int  1 0 0 0 0 0 0 0 1 0 ...
    ##  $ Dependents_Yes                       : int  0 0 0 0 0 0 1 0 0 1 ...
    ##  $ PhoneService_Yes                     : int  0 1 1 0 1 1 1 0 1 1 ...
    ##  $ InternetService_Fiber optic          : int  0 0 0 0 1 1 1 0 1 0 ...
    ##  $ InternetService_No                   : int  0 0 0 0 0 0 0 0 0 0 ...
    ##  $ Contract_One year                    : int  0 1 0 1 0 0 0 0 0 1 ...
    ##  $ Contract_Two year                    : int  0 0 0 0 0 0 0 0 0 0 ...
    ##  $ PaperlessBilling_Yes                 : int  1 0 1 0 1 1 1 0 1 0 ...
    ##  $ PaymentMethod_Credit card (automatic): int  0 0 0 0 0 0 1 0 0 0 ...
    ##  $ PaymentMethod_Electronic check       : int  1 0 0 0 1 1 0 0 1 0 ...
    ##  $ PaymentMethod_Mailed check           : int  0 1 1 0 0 0 0 1 0 0 ...
    ##  $ Churn_Yes                            : int  0 0 1 0 1 1 0 0 1 0 ...

    library(dplyr)

    ## 
    ## Attaching package: 'dplyr'

    ## The following objects are masked from 'package:stats':
    ## 
    ##     filter, lag

    ## The following objects are masked from 'package:base':
    ## 
    ##     intersect, setdiff, setequal, union

    df_clean %>% group_by(Churn_Yes) %>% tally

    ## # A tibble: 2 x 2
    ##   Churn_Yes     n
    ##       <int> <int>
    ## 1         0  5163
    ## 2         1  1869

Data cleaning is now finished.  
For the purposes of this post, I want to explore the models in great
detail. To avoid making this post too long, I will skip data
exploration. This is a very important and not a trivial step. I would
encourage you to perform your own on this data set. Some exploration
might be to see boxplots of the bill amounts by demographics. Can follow
this up with anova testing too!

### Modeling

We wish to explore both logistic regression and neural net modeling.

    library(keras)
    df_nn <- df_clean
    df_nn <- as.matrix(df_nn)
    dimnames(df_nn) <- NULL

    df_nn[, 1] <- scale(df_nn[,1])
    df_nn[, 2] <- scale(df_nn[,2])

    summary(df_nn)

    ##        V1                V2                V3               V4        
    ##  Min.   :-1.2802   Min.   :-0.9990   Min.   :0.0000   Min.   :0.0000  
    ##  1st Qu.:-0.9542   1st Qu.:-0.8302   1st Qu.:0.0000   1st Qu.:0.0000  
    ##  Median :-0.1394   Median :-0.3908   Median :1.0000   Median :0.0000  
    ##  Mean   : 0.0000   Mean   : 0.0000   Mean   :0.5047   Mean   :0.1624  
    ##  3rd Qu.: 0.9199   3rd Qu.: 0.6668   3rd Qu.:1.0000   3rd Qu.:0.0000  
    ##  Max.   : 1.6125   Max.   : 2.8241   Max.   :1.0000   Max.   :1.0000  
    ##        V5               V6               V7               V8        
    ##  Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  
    ##  1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:1.0000   1st Qu.:0.0000  
    ##  Median :0.0000   Median :0.0000   Median :1.0000   Median :0.0000  
    ##  Mean   :0.4825   Mean   :0.2985   Mean   :0.9033   Mean   :0.4403  
    ##  3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  
    ##  Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  
    ##        V9              V10              V11              V12        
    ##  Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  
    ##  1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000  
    ##  Median :0.0000   Median :0.0000   Median :0.0000   Median :1.0000  
    ##  Mean   :0.2162   Mean   :0.2093   Mean   :0.2396   Mean   :0.5927  
    ##  3rd Qu.:0.0000   3rd Qu.:0.0000   3rd Qu.:0.0000   3rd Qu.:1.0000  
    ##  Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  
    ##       V13              V14              V15              V16        
    ##  Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  
    ##  1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000  
    ##  Median :0.0000   Median :0.0000   Median :0.0000   Median :0.0000  
    ##  Mean   :0.2163   Mean   :0.3363   Mean   :0.2281   Mean   :0.2658  
    ##  3rd Qu.:0.0000   3rd Qu.:1.0000   3rd Qu.:0.0000   3rd Qu.:1.0000  
    ##  Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000

In this bit of code above, I am formatting the data for the neural
network. The independent variables have all been normalized or converted
to binary.

Now we split this data into a test set and a training set.

    #Splitting Dataset into test and train
    library(caTools)
    set.seed(123)
    ind <- sample(2, nrow(df_nn), replace = T, prob = c(.8, .2))
    train <- df_nn[ind==1, 1:15]
    test <- df_nn[ind==2, 1:15]
    train_y<- df_nn[ind==1, 16]
    test_y <- df_nn[ind==2, 16]

Now we are ready to build the neural network. I will start with two
hidden layers and an output layer. This should be sufficient to maximize
the predictive power without overtraining.

    nn <- keras_model_sequential()

    nn %>%
      layer_dense(units=10, activation = "relu", input_shape = 15) %>%
      layer_dropout(rate = .30) %>%
      layer_dense(units=10, activation = "relu") %>%
      layer_dropout(rate = .20) %>%
      layer_dense(units = 1, activation = "sigmoid") %>%
        compile(loss = 'binary_crossentropy',
              optimizer = optimizer_adam(lr= 0.01, decay = 1e-6), metrics = c('accuracy'))

The model that we have created has four hidden layers and an output
layer. The first two hidden layers have 10 nodes each. The number of
parameters in the first layer is 10 \* 15 + 10 or 160. The second layer
has 10 \* 10 + 10 or 110 and the last layer has 1\*10 + 1 or 11
parameters. Thus the total model has 281 parameters. I have included a
dropout layer between each hidden layer and chose dropout rates of .3
and .2 respectively. This is to avoid overfitting the model. I have
selected our loss function to be binary cross-entropy. This is because
we have a binary dependent variable. I have selected the adam optiimizer
which has become more popuar than sgd over the past 5 years.

Now we fit the model to our data.

    model <- nn %>%
             fit(train,
                 train_y,
                 epoch = 50,
                 batch_size = 128,
                 validation_split = 0.25)
    plot(model)

    ## `geom_smooth()` using formula 'y ~ x'

<img src="{{ site.url }}{{ site.baseurl }}/images/plot 2 nn.jpg" alt="">

We have run and fit our model. You will notice that our validation and
our training accuracy run pretty tight. This is a good indication that e
did not overfit the model. Let us use this model to predict the churn of
our test set.

    testingmodel <- nn %>%
             evaluate(test, test_y)
    print(as.data.frame(testingmodel))

    ##        loss  accuracy
    ## 1 0.4519465 0.7893602

    pred <- nn %>%
             predict_classes(test)
    cm <- table(Predicted = pred, Actual = test_y)
    print(cm)

    ##          Actual
    ## Predicted   0   1
    ##         0 932 211
    ##         1  82 166

This confusion matrix shows that we would have accurately predicted that
166 customers would leave our service. If a marketing campaign were
tasked with reaching out to a specific group, they would have reached
248 people, 166 of which would have left and the other 82 would have
stayed regarless. They would have missed 211 people. We correctly
identified 932 customers who did not discountinue services.
